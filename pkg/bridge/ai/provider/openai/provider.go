package openai

import (
	"bytes"
	"encoding/json"
	"errors"
	"fmt"
	"io"
	"net/http"
	"os"
	"sync"

	_ "github.com/joho/godotenv/autoload"
	"github.com/yomorun/yomo/ai"
	"github.com/yomorun/yomo/core/ylog"
)

const APIEndpoint = "https://api.openai.com/v1/chat/completions"

var (
	fns               sync.Map
	ErrNoFunctionCall = errors.New("no function call")
)

// Message
type ChatCompletionMessage struct {
	Role    string `json:"role"`
	Content string `json:"content"`
	// - https://github.com/openai/openai-python/blob/main/chatml.md
	// - https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb
	Name string `json:"name,omitempty"`
	// MultiContent []ChatMessagePart
	// For Role=assistant prompts this may be set to the tool calls generated by the model, such as function calls.
	ToolCalls  []ai.ToolCall `json:"tool_calls,omitempty"`
	ToolCallID string        `json:"tool_call_id,omitempty"`
}

// RequestBody is the request body
type ReqBody struct {
	Model    string                  `json:"model"`
	Messages []ChatCompletionMessage `json:"messages"`
	Tools    []ai.ToolCall           `json:"tools"` // chatCompletionTool
	// ToolChoice string    `json:"tool_choice"` // chatCompletionFunction
}

// Resp is the response body
type RespBody struct {
	ID                string       `json:"id"`
	Object            string       `json:"object"`
	Created           int          `json:"created"`
	Model             string       `json:"model"`
	Choices           []RespChoice `json:"choices"`
	Usage             RespUsage    `json:"usage"`
	SystemFingerprint string       `json:"system_fingerprint"`
}

// RespMessage is the message in Response
type RespMessage struct {
	Role      string        `json:"role"`
	Content   string        `json:"content"`
	ToolCalls []ai.ToolCall `json:"tool_calls"`
}

// RespChoice is used to indicate the choice in Response by `FinishReason`
type RespChoice struct {
	FinishReason string                `json:"finish_reason"`
	Index        int                   `json:"index"`
	Message      ChatCompletionMessage `json:"message"`
}

// RespUsage is the token usage in Response
type RespUsage struct {
	PromptTokens     int `json:"prompt_tokens"`
	CompletionTokens int `json:"completion_tokens"`
	TotalTokens      int `json:"total_tokens"`
}

// OpenAIProvider is the provider for OpenAI
type OpenAIProvider struct {
	// APIKey is the API key for OpenAI
	APIKey string
	// Model is the model for OpenAI
	// eg. "gpt-3.5-turbo-1106", "gpt-4-turbo-preview", "gpt-4-vision-preview", "gpt-4"
	Model string
}

type connectedFn struct {
	connID string
	tag    uint32
	tc     ai.ToolCall
}

func init() {
	fns = sync.Map{}
}

// NewOpenAIProvider creates a new OpenAIProvider
func NewOpenAIProvider(apiKey string, model string) *OpenAIProvider {
	return &OpenAIProvider{
		APIKey: apiKey,
		Model:  model,
	}
}

// New creates a new OpenAIProvider
func New() *OpenAIProvider {
	return &OpenAIProvider{
		APIKey: os.Getenv("OPENAI_API_KEY"),
		Model:  os.Getenv("OPENAI_MODEL"),
	}
}

// Name returns the name of the provider
func (p *OpenAIProvider) Name() string {
	return "openai"
}

// GetChatCompletions get chat completions for ai service
func (p *OpenAIProvider) GetChatCompletions(userInstruction string) (*ai.ChatCompletionsResponse, error) {
	isEmpty := true
	fns.Range(func(key, value interface{}) bool {
		isEmpty = false
		return false
	})

	if isEmpty {
		ylog.Error(ErrNoFunctionCall.Error())
		return &ai.ChatCompletionsResponse{Content: "no toolcalls"}, ErrNoFunctionCall
	}

	// messages
	messages := []ChatCompletionMessage{
		{Role: "system", Content: `You are a very helpful assistant. Your job is to choose the best possible action to solve the user question or task. Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous. If you don't know the answer, stop the conversation by saying "no func call".`},
		{Role: "user", Content: userInstruction},
	}

	// prepare tools
	toolCalls := make([]ai.ToolCall, 0)
	fns.Range(func(_, value any) bool {
		fn := value.(*connectedFn)
		toolCalls = append(toolCalls, fn.tc)
		return true
	})

	body := ReqBody{Model: p.Model, Messages: messages, Tools: toolCalls}

	ylog.Debug("request", "tools", len(toolCalls), "messages", messages)

	jsonBody, err := json.Marshal(body)
	if err != nil {
		return nil, err
	}

	req, err := http.NewRequest("POST", APIEndpoint, bytes.NewBuffer(jsonBody))
	if err != nil {
		return nil, err
	}
	req.Header.Set("Content-Type", "application/json")
	// OpenAI authentication
	req.Header.Set("Authorization", fmt.Sprintf("Bearer %s", p.APIKey))

	client := &http.Client{}
	resp, err := client.Do(req)
	if err != nil {
		return nil, err
	}
	defer resp.Body.Close()

	respBody, err := io.ReadAll(resp.Body)
	if err != nil {
		return nil, err
	}
	ylog.Debug("response", "body", respBody)

	// slog.Info("response body", "body", string(respBody))
	if resp.StatusCode >= 400 {
		return nil, fmt.Errorf("ai response status code is %d", resp.StatusCode)
	}

	var respBodyStruct RespBody
	err = json.Unmarshal(respBody, &respBodyStruct)
	if err != nil {
		return nil, err
	}
	// fmt.Println(string(respBody))
	// TODO: record usage
	// usage := respBodyStruct.Usage
	// log.Printf("Token Usage: %+v\n", usage)

	choice := respBodyStruct.Choices[0]
	ylog.Debug(">>finish_reason", "reason", choice.FinishReason)

	calls := respBodyStruct.Choices[0].Message.ToolCalls
	content := respBodyStruct.Choices[0].Message.Content

	ylog.Debug("--response calls", "calls", calls)

	result := &ai.ChatCompletionsResponse{}
	if len(calls) == 0 {
		result.Content = content
		return result, ErrNoFunctionCall
	}

	// functions may be more than one
	// slog.Info("tool calls", "calls", calls, "mapTools", mapTools)
	for _, call := range calls {
		fns.Range(func(key, value interface{}) bool {
			fn := value.(*connectedFn)
			if fn.tc.Equal(&call) {
				// if result.Functions == nil {
				// 	result.Functions = make(map[uint32][]*ai.FunctionDefinition)
				// }
				// Use toolCalls because tool_id is required in the following llm request
				if result.ToolCalls == nil {
					result.ToolCalls = make(map[uint32][]*ai.ToolCall)
				}
				// result.Functions[fn.tag] = append(result.Functions[fn.tag], call.Function)

				// fix: should push the `call` instead of `call.Function` as describes in
				// https://cookbook.openai.com/examples/function_calling_with_an_openapi_spec
				// Create a new variable to hold the current call
				currentCall := call
				result.ToolCalls[fn.tag] = append(result.ToolCalls[fn.tag], &currentCall)
			}
			return true
		})
	}

	// sfn maybe disconnected, so we need to check if there is any function call
	if len(result.ToolCalls) == 0 {
		return nil, ErrNoFunctionCall
	}
	return result, nil
}

// RegisterFunction register function
func (p *OpenAIProvider) RegisterFunction(tag uint32, functionDefinition *ai.FunctionDefinition, connID string) error {
	fns.Store(connID, &connectedFn{
		connID: connID,
		tag:    tag,
		tc: ai.ToolCall{
			Type:     "function",
			Function: functionDefinition,
		},
	})

	return nil
}

// UnregisterFunction unregister function
// Be careful: a function can have multiple instances, remove the offline instance only.
func (p *OpenAIProvider) UnregisterFunction(name string, connID string) error {
	fns.Delete(connID)
	return nil
}

// ListToolCalls list tool functions
func (p *OpenAIProvider) ListToolCalls() (map[uint32]ai.ToolCall, error) {
	tmp := make(map[uint32]ai.ToolCall)
	fns.Range(func(key, value any) bool {
		fn := value.(*connectedFn)
		tmp[fn.tag] = fn.tc
		return true
	})

	return tmp, nil
}

// GetOverview get overview for ai service
func (p *OpenAIProvider) GetOverview() (*ai.OverviewResponse, error) {
	isEmpty := true
	fns.Range(func(key, value any) bool {
		isEmpty = false
		return false
	})

	result := &ai.OverviewResponse{
		Functions: make(map[uint32]*ai.FunctionDefinition),
	}

	if isEmpty {
		return result, nil
	}

	fns.Range(func(key, value any) bool {
		fn := value.(*connectedFn)
		result.Functions[fn.tag] = fn.tc.Function
		return true
	})

	return result, nil
}
